{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPd4ttUINQ4LNoiD6gNgdka",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jordan-dsouza/SlashMarkInternship/blob/main/MNIST_SlashMark/CNN_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "h6M5rSnDV9a-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7_vwVN5VUh7"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dense\n",
        "\n",
        "class CNN:\n",
        "    @staticmethod\n",
        "    def build(width, height, depth, total_classes, Saved_Weights_Path=None):\n",
        "        #Initialize the Model:\n",
        "        model = Sequential()\n",
        "\n",
        "        #First CONV => RELU => POOL Layer:\n",
        "        model.add(Conv2D(20, 5, 5, border_mode=\"same\", input_shape=(depth, height, width)))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), dim_ordering=\"th\"))\n",
        "\n",
        "        #Second CONV => RELU => POOL Layer:\n",
        "        model.add(Conv2D(50, 5, 5, border_mode=\"same\"))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), dim_ordering=\"th\"))\n",
        "\n",
        "        #Third CONV => RELU => POOL Layer:\n",
        "        #Convolution -> ReLU Activation Function -> Pooling Layer:\n",
        "        model.add(Conv2D(100, 5, 5, border_mode=\"same\"))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), dim_ordering=\"th\"))\n",
        "\n",
        "        #FC => RELU layers:\n",
        "        #Fully Connected Layer -> ReLU Activation Function:\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(500))\n",
        "        model.add(Activation(\"relu\"))\n",
        "\n",
        "        #Using Softmax Classifier for Linear Classification:\n",
        "        model.add(Dense(total_classes))\n",
        "        model.add(Activation(\"softmax\"))\n",
        "\n",
        "        #If the saved_weights file is already present i.e model is pre-trained, load that weights:\n",
        "        if Saved_Weights_Path is not None:\n",
        "            model.load_weights(Saved_Weights_Path)\n",
        "        return model\n",
        "# --------------------------------- EOC ------------------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import argparse\n",
        "import cv2\n",
        "import tensorflow\n",
        "#from cnn.neural_network import CNN\n",
        "from keras.utils import np_utils\n",
        "from keras.optimizers import SGD\n",
        "from tensorflow import keras\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "#Parse the Arguments:\n",
        "ap = argparse.ArgumentParser()\n",
        "ap.add_argument(\"-s\", \"--save_model\", type=int, default=-1)\n",
        "ap.add_argument(\"-l\", \"--load_model\", type=int, default=-1)\n",
        "ap.add_argument(\"-w\", \"--save_weights\", type=str)\n",
        "args = vars(ap.parse_args())\n",
        "\n",
        "#Read/Download MNIST Dataset:\n",
        "print('Loading MNIST Dataset...')\n",
        "# dataset = fetch_mldata('MNIST Original')\n",
        "dataset = fetch_openml('mnist_784')\n",
        "\n",
        "#Read the MNIST data as array of 784 pixels and convert to 28x28 image matrix:\n",
        "mnist_data = dataset.data.reshape((dataset.data.shape[0], 28, 28))\n",
        "mnist_data = mnist_data[:, np.newaxis, :, :]\n",
        "\n",
        "#Divide data into testing and training sets:\n",
        "train_img, test_img, train_labels, test_labels = train_test_split(mnist_data/255.0, dataset.target.astype(\"int\"), test_size=0.1)\n",
        "\n",
        "#Now each image rows and columns are of 28x28 matrix type:\n",
        "img_rows, img_columns = 28, 28\n",
        "\n",
        "#Transform training and testing data to 10 classes in range [0,classes] ; num. of classes = 0 to 9 = 10 classes:\n",
        "total_classes = 10\t\t\t# 0 to 9 labels\n",
        "train_labels = np_utils.to_categorical(train_labels, 10)\n",
        "test_labels = np_utils.to_categorical(test_labels, 10)\n",
        "\n",
        "#Defining and compiling the SGD optimizer and CNN model:\n",
        "print('\\n Compiling model...')\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "clf = CNN.build(width=28, height=28, depth=1, total_classes=10, Saved_Weights_Path=args[\"save_weights\"] if args[\"load_model\"] > 0 else None)\n",
        "clf.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"accuracy\"])\n",
        "\n",
        "#Initially train and test the model; If weight saved already, load the weights using arguments:\n",
        "b_size = 128\t\t# Batch size\n",
        "num_epoch = 20\t\t# Number of epochs\n",
        "verb = 1\t\t\t# Verbose\n",
        "\n",
        "#If weights saved and argument load_model; Load the pre-trained model:\n",
        "if args[\"load_model\"] < 0:\n",
        "\tprint('\\nTraining the Model...')\n",
        "\tclf.fit(train_img, train_labels, batch_size=b_size, epochs=num_epoch,verbose=verb)\n",
        "\n",
        "\t#Evaluate accuracy and loss function of test data:\n",
        "\tprint('Evaluating Accuracy and Loss Function...')\n",
        "\tloss, accuracy = clf.evaluate(test_img, test_labels, batch_size=128, verbose=1)\n",
        "\tprint('Accuracy of Model: {:.2f}%'.format(accuracy * 100))\n",
        "\n",
        "\n",
        "#Save the pre-trained model:\n",
        "if args[\"save_model\"] > 0:\n",
        "\tprint('Saving weights to file...')\n",
        "\tclf.save_weights(args[\"save_weights\"], overwrite=True)\n",
        "\n",
        "\n",
        "#Show the images using OpenCV and making random selections:\n",
        "for num in np.random.choice(np.arange(0, len(test_labels)), size=(5,)):\n",
        "\t#Predict the label of digit using CNN:\n",
        "\tprobs = clf.predict(test_img[np.newaxis, num])\n",
        "\tprediction = probs.argmax(axis=1)\n",
        "\n",
        "\t#Resize the Image to 100x100 from 28x28 for better view:\n",
        "\timage = (test_img[num][0] * 255).astype(\"uint8\")\n",
        "\timage = cv2.merge([image] * 3)\n",
        "\timage = cv2.resize(image, (100, 100), interpolation=cv2.INTER_LINEAR)\n",
        "\tcv2.putText(image, str(prediction[0]), (5, 20),cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
        "\n",
        "\t#Show and print the Actual Image and Predicted Label Value:\n",
        "\tprint('Predicted Label: {}, Actual Value: {}'.format(prediction[0],np.argmax(test_labels[num])))\n",
        "\n",
        "#---------------------- EOC ---------------------\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "lA5uMUpVVfLP",
        "outputId": "184dae05-0ba9-4dbd-e680-e3c5ebccb987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'np_utils' from 'keras.utils' (/usr/local/lib/python3.10/dist-packages/keras/utils/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-881941492841>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#from cnn.neural_network import CNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'np_utils' from 'keras.utils' (/usr/local/lib/python3.10/dist-packages/keras/utils/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Read/Download MNIST Dataset:\n",
        "print('Loading MNIST Dataset...')\n",
        "mnist_data = fetch_openml('mnist_784', version=1)\n",
        "mnist_data.data = mnist_data.data.astype('float32')\n",
        "mnist_data.data /= 255\n",
        "\n",
        "# Divide data into testing and training sets:\n",
        "train_img, test_img, train_labels, test_labels = train_test_split(\n",
        "    mnist_data.data, mnist_data.target, test_size=0.1)\n",
        "# Convert DataFrame to numpy array\n",
        "train_img = train_img.to_numpy()\n",
        "test_img = test_img.to_numpy()\n",
        "\n",
        "# Reshape data to match the input shape expected by Keras\n",
        "train_img = train_img.reshape(train_img.shape[0], 28, 28, 1)\n",
        "test_img = test_img.reshape(test_img.shape[0], 28, 28, 1)\n",
        "\n",
        "# Transform training and testing labels to categorical\n",
        "train_labels = to_categorical(train_labels, 10)\n",
        "test_labels = to_categorical(test_labels, 10)\n",
        "\n",
        "# Define and compile the SGD optimizer and CNN model:\n",
        "print('Compiling model...')\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model:\n",
        "print('Training the Model...')\n",
        "model.fit(train_img, train_labels, batch_size=128, epochs=20, verbose=1)\n",
        "\n",
        "# Evaluate accuracy and loss function on test data:\n",
        "print('Evaluating Accuracy and Loss Function...')\n",
        "loss, accuracy = model.evaluate(test_img, test_labels, batch_size=128, verbose=1)\n",
        "print('Accuracy of Model: {:.2f}%'.format(accuracy * 100))\n",
        "\n",
        "# Save the model:\n",
        "print('Saving model to file...')\n",
        "model.save(\"mnist_model.h5\")\n",
        "\n",
        "# Show the images using OpenCV and making random selections:\n",
        "for num in np.random.choice(np.arange(0, len(test_labels)), size=(5,)):\n",
        "    # Predict the label of digit using CNN:\n",
        "    probs = model.predict(test_img[np.newaxis, num])\n",
        "    prediction = probs.argmax(axis=1)\n",
        "\n",
        "    # Resize the Image to 100x100 for better view:\n",
        "    image = (test_img[num][:, :, 0] * 255).astype(\"uint8\")\n",
        "    image = cv2.merge([image] * 3)\n",
        "    image = cv2.resize(image, (100, 100), interpolation=cv2.INTER_LINEAR)\n",
        "    cv2.putText(image, str(prediction[0]), (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
        "\n",
        "    # Show and print the Actual Image and Predicted Label Value:\n",
        "    print('Predicted Label: {}, Actual Value: {}'.format(prediction[0], np.argmax(test_labels[num])))\n",
        "    cv2.imshow(\"Digit\", image)\n",
        "    cv2.waitKey(0)\n",
        "\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8XLxy_nxXGlz",
        "outputId": "93588ced-53a1-4976-aa8c-21dc60815b55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading MNIST Dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiling model...\n",
            "Training the Model...\n",
            "Epoch 1/20\n",
            "493/493 [==============================] - 28s 54ms/step - loss: 0.3553 - accuracy: 0.8960\n",
            "Epoch 2/20\n",
            "493/493 [==============================] - 26s 53ms/step - loss: 0.1469 - accuracy: 0.9561\n",
            "Epoch 3/20\n",
            "493/493 [==============================] - 26s 53ms/step - loss: 0.1039 - accuracy: 0.9685\n",
            "Epoch 4/20\n",
            "493/493 [==============================] - 27s 56ms/step - loss: 0.0816 - accuracy: 0.9750\n",
            "Epoch 5/20\n",
            "493/493 [==============================] - 26s 53ms/step - loss: 0.0663 - accuracy: 0.9794\n",
            "Epoch 6/20\n",
            "493/493 [==============================] - 26s 54ms/step - loss: 0.0560 - accuracy: 0.9836\n",
            "Epoch 7/20\n",
            "493/493 [==============================] - 26s 53ms/step - loss: 0.0471 - accuracy: 0.9857\n",
            "Epoch 8/20\n",
            "493/493 [==============================] - 27s 54ms/step - loss: 0.0408 - accuracy: 0.9880\n",
            "Epoch 9/20\n",
            "493/493 [==============================] - 27s 54ms/step - loss: 0.0364 - accuracy: 0.9888\n",
            "Epoch 10/20\n",
            "493/493 [==============================] - 28s 56ms/step - loss: 0.0328 - accuracy: 0.9897\n",
            "Epoch 11/20\n",
            "493/493 [==============================] - 27s 54ms/step - loss: 0.0282 - accuracy: 0.9913\n",
            "Epoch 12/20\n",
            "493/493 [==============================] - 28s 56ms/step - loss: 0.0250 - accuracy: 0.9927\n",
            "Epoch 13/20\n",
            "493/493 [==============================] - 27s 55ms/step - loss: 0.0232 - accuracy: 0.9934\n",
            "Epoch 14/20\n",
            "493/493 [==============================] - 27s 55ms/step - loss: 0.0198 - accuracy: 0.9942\n",
            "Epoch 15/20\n",
            "493/493 [==============================] - 27s 56ms/step - loss: 0.0175 - accuracy: 0.9952\n",
            "Epoch 16/20\n",
            "493/493 [==============================] - 27s 54ms/step - loss: 0.0156 - accuracy: 0.9955\n",
            "Epoch 17/20\n",
            "493/493 [==============================] - 27s 55ms/step - loss: 0.0143 - accuracy: 0.9963\n",
            "Epoch 18/20\n",
            "493/493 [==============================] - 26s 53ms/step - loss: 0.0131 - accuracy: 0.9967\n",
            "Epoch 19/20\n",
            "493/493 [==============================] - 27s 54ms/step - loss: 0.0111 - accuracy: 0.9975\n",
            "Epoch 20/20\n",
            "493/493 [==============================] - 27s 54ms/step - loss: 0.0097 - accuracy: 0.9977\n",
            "Evaluating Accuracy and Loss Function...\n",
            "55/55 [==============================] - 1s 16ms/step - loss: 0.0438 - accuracy: 0.9864\n",
            "Accuracy of Model: 98.64%\n",
            "Saving model to file...\n",
            "1/1 [==============================] - 0s 95ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Label: 9, Actual Value: 9\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "DisabledFunctionError",
          "evalue": "cv2.imshow() is disabled in Colab, because it causes Jupyter sessions\nto crash; see https://github.com/jupyter/notebook/issues/3935.\nAs a substitution, consider using\n  from google.colab.patches import cv2_imshow\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDisabledFunctionError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-0ac0f51a79e7>\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# Show and print the Actual Image and Predicted Label Value:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted Label: {}, Actual Value: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Digit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_import_hooks/_cv2.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mDisabledFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDisabledFunctionError\u001b[0m: cv2.imshow() is disabled in Colab, because it causes Jupyter sessions\nto crash; see https://github.com/jupyter/notebook/issues/3935.\nAs a substitution, consider using\n  from google.colab.patches import cv2_imshow\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_snippet",
                "actionText": "Search Snippets for cv2.imshow",
                "snippetFilter": "cv2.imshow"
              }
            ]
          }
        }
      ]
    }
  ]
}